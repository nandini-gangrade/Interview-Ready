## Big O

If you already *truly* understand Big O, you can skip this.
But if Big O feels fuzzy, scary, or like something you memorized once and forgotâ€”this is for you.

### First things first: what is Big O?

Big O is just a **way to describe how an algorithm behaves as the input grows**.

Thatâ€™s it.
Not how fast it runs on *your* laptop.
Not how many milliseconds it takes today.

It answers questions like:

* What happens if my input becomes **10x bigger**?
* Will my code still work fine, or will it crawl and die?

Big O talks about **computational complexity**, which has two parts:

1. **Time Complexity** â†’ How much *time* the algorithm takes as input size grows
2. **Space Complexity** â†’ How much *extra memory* the algorithm uses as input size grows

Most people focus more on **time**, but **space also matters**, especially in interviews and real systems.

---

## How complexity actually works (without math headache)

Complexity is described using a **function**.

But a function of *what*?

ğŸ‘‰ Of the **input size**.

As programmers, **we define the variables**.
The most common one youâ€™ll see is:

* `n` â†’ length of an array or string

Example:

* Array with 10 elements â†’ `n = 10`
* Array with 1 million elements â†’ `n = 1,000,000`

### Important interview assumption (very important)

Technically:

* Bigger integers take more time to add, multiply, or print.

But in interviews and real-world discussions, **we ignore that** because the difference is tiny and impractical to track.

So if an array of integers is given:

* We **only care about how many elements**, not how big each number is.

Yes, you *could* introduce another variable like average value of integersâ€”but nobody does that. Donâ€™t overthink it.

---

## What does `O(...)` actually mean?

Big O wraps the function in a capital **O**, like:

* `O(n)`
* `O(nÂ²)`
* `O(log n)`

If you see something like `O(n + m)`:

* It usually means there are **two inputs**
* `n` = size of first input
* `m` = size of second input

Nothing magicalâ€”just variables we define.

---

## The golden rules of Big O (super important)

### 1ï¸âƒ£ Constants are ignored

Why?

Because Big O describes **growth as input â†’ infinity**.

So:

* `O(2n)` â†’ `O(n)`
* `O(100n)` â†’ `O(n)`

If you double the input:

* Both `n` and `100n` grow **linearly**
* Growth pattern is the same

Even if one algorithm is 20x faster in practice, **Big O only cares about scaling**, not raw speed.

---

### 2ï¸âƒ£ Dominant terms win

When multiple terms exist, the **largest-growing term dominates**.

Example:

* `O(nÂ² + n + 1)` â†’ `O(nÂ²)`

Why?

* When `n` is huge, `nÂ²` completely overshadows `n` and constants.

---

## Why interviewers care so much about this

Interviewers ask for time and space complexity because:

* They want to see if you **understand your own code**
* Not just copied or memorized a solution
* It shows you can **reason about performance**
* It helps you identify **bottlenecks and optimizations**

This is a *core SDE skill*, not theory for theoryâ€™s sake.

---

## Best, average, and worst cases

When talking about complexity, we usually consider:

1. **Best case**
2. **Average case**
3. **Worst case**

Most algorithms have the same complexity in all cases, but some donâ€™t.

ğŸ‘‰ **Never quote the best case alone.**
ğŸ‘‰ **Worst case is usually the safest and most correct choice**, especially in interviews.

But you should *know* the difference.

---

## The best possible complexity

### `O(1)` â€” Constant Time / Constant Space

This means:

* The algorithm always takes the **same time**
* Uses the **same memory**
* No matter how big the input is

âš ï¸ Important:

* `O(1)` does **not** mean â€œfastâ€
* It just means **independent of input size**

---

## Letâ€™s analyze time complexity with examples

### Example 1: Simple loop

```java
for (int num : arr) {
    print(num);
}
```

* Loop runs `n` times
* Each print is `O(1)`
* Total time â†’ **`O(n)`**

---

### Example 2: Loop with a big constant

```java
for (int num : arr) {
    for (int i = 0; i < 500000; i++) {
        print(num);
    }
}
```

* Inner loop runs 500,000 times â†’ constant
* Outer loop runs `n` times
* Total time â†’ **`O(n)`**

Yes, this is *much slower* in real life than the previous one.
But **Big O ignores constants**, so both are `O(n)`.

ğŸ‘‰ In interviews, say:

> â€œTime complexity is O(n), but practically this is slower due to the large constant.â€

That shows maturity.

---

### Example 3: Nested loops over same array

```java
for (int num : arr) {
    for (int num2 : arr) {
        print(num * num2);
    }
}
```

* Outer loop â†’ `n`
* Inner loop â†’ `n`
* Total â†’ **`O(nÂ²)`**

Classic quadratic time.

---

### Example 4: Two independent inputs

```java
for (int num : arr) {
    print(num);
}

for (int num : arr) {
    print(num);
}

for (int num : arr2) {
    print(num);
}
```

* First two loops â†’ `O(n)`
* Third loop â†’ `O(m)`
* Total â†’ **`O(n + m)`**

---

### Example 5: Dependent loops

```java
for (int i = 0; i < n; i++) {
    for (int j = i; j < n; j++) {
        print(arr[i] + arr[j]);
    }
}
```

* Inner loop runs:

  * `n` times first
  * `n-1` times next
  * ...
* Total operations â†’ `1 + 2 + 3 + ... + n`
* That sum = `n(n + 1)/2`
* Big O â†’ **`O(nÂ²)`**

---

## Logarithmic time â€” the fast stuff

### `O(log n)`

This is **extremely efficient**.

A logarithm is the inverse of an exponent.

Youâ€™ll usually see log base 2, but **the base doesnâ€™t matter** in Big O.

### What does `O(log n)` mean intuitively?

It means:

> â€œAt every step, the problem size shrinks by a fixed percentage.â€

### Example: Binary Search

* Start with `n` elements
* Step 1 â†’ `n / 2`
* Step 2 â†’ `n / 4`
* Step 3 â†’ `n / 8`
* â€¦

Youâ€™re cutting the input in half every time â†’ **logarithmic time**

Thatâ€™s why binary search and efficient sorting algorithms are so powerful.

---

## Space complexity (memory usage)

We **donâ€™t count input space**
We usually **donâ€™t count output space** unless asked

We only count **extra memory allocated by the algorithm**.

---

### Example 1: No extra memory

```java
for (int num : arr) {
    print(num);
}
```

* Only one variable `num`
* Space â†’ **`O(1)`**

---

### Example 2: New array of same size

```java
Array doubledNums = new int[];

for (int num : arr) {
    doubledNums.add(num * 2);
}
```

* Stores `n` elements
* Space â†’ **`O(n)`**

---

### Example 3: Only 1% of input

```java
Array nums = new int[];
int oneHundredth = n / 100;

for (int i = 0; i < oneHundredth; i++) {
    nums.add(arr[i]);
}
```

* Stores `n/100` elements
* Big O ignores constants â†’ **`O(n)`**

---

### Example 4: 2D grid

```java
Array grid = new int[n][m];

for (int i = 0; i < n; i++) {
    for (int j = 0; j < m; j++) {
        grid[i][j] = arr[i] * arr2[j];
    }
}
```

* Grid size = `n Ã— m`
* Space â†’ **`O(nm)`**

---

## Where and why youâ€™ll actually use Big O

### Youâ€™ll use Big O when:

* Solving **DSA problems**
* Comparing **multiple approaches**
* Explaining your solution in **interviews**
* Designing **scalable systems**
* Avoiding performance disasters in production

### Why it matters:

* Helps you **choose the right approach**
* Prevents timeouts and memory issues
* Shows interviewers you **think like an engineer**, not a code typist

---

## Short recap â€” What Iâ€™ve learned (quick recall)

* Big O describes **how algorithms scale**
* Time complexity â†’ execution time vs input size
* Space complexity â†’ extra memory vs input size
* Ignore **constants**
* Focus on **dominant terms**
* `O(1)` = constant, not necessarily fast
* `O(n)` = linear
* `O(nÂ²)` = nested loops
* `O(log n)` = input shrinks each step
* Worst case is usually the safest to mention
* Big O helps in **interviews and real-world systems**
